# RAG_QNA
Document-QA using LLAMA-3.1.70B - RAG - ChromaDB
Overview
This project implements a Retrieval-Augmented Generation (RAG) system utilizing Meta's LLAMA 3.1 70B model, integrated with ChromaDB as the vector store, and LangChain. The LLAMA 3.1 model is loaded with Groq, showcasing the use of the Chroma library to build and manage a document database. The system leverages LLAMA language models for efficient document retrieval and question answering (QA). Additionally, the project integrates HuggingFace embeddings and Groq models to enhance processing and querying of textual data.

Features
Document Database Creation: Utilizes Chroma to store and manage documents efficiently.
Embedding Generation: Uses HuggingFace embeddings to convert documents into vector representations.
Vector Database: Constructs a vector database for fast and accurate retrieval.
RetrievalQA Chain: Implements a QA system using the LLAMA language model to answer queries based on document content.
Project Structure
Load Data and Create Database: This component is responsible for loading textual data, generating embeddings, and creating the document database.
Query Processing: Processes user queries using a retrieval-based approach to fetch relevant information from the document database.
Model Integration: Integrates the LLAMA language model to provide sophisticated and contextually accurate answers to user queries.
How It Works
Embeddings Creation:

The project starts by generating embeddings for the input documents using the HuggingFaceEmbeddings class. These embeddings transform textual data into numerical vectors suitable for vector operations.
Persisting Data:

A directory named doc_db is created to store the vectorized documents. This ensures that embeddings and documents are preserved and can be reused without recalculating embeddings.
Vector Database Setup:

The Chroma library is used to build a vector database from the document embeddings, supporting efficient similarity searches and retrieval operations.
Retriever Setup:

A retriever is initialized from the vector database to fetch relevant documents based on the similarity of query embeddings.
LLAMA Model Integration:

The LLAMA language model, specifically the ChatGroq implementation, is integrated. Configured with the llama-3.1-70b-versatile model and a temperature setting of 0, it ensures deterministic and precise responses.
QA Chain Creation:

A QA chain is constructed using the RetrievalQA class. This chain combines the retriever and the language model to process queries and deliver answers along with the source documents.
Query Execution:

The QA chain is invoked with sample queries. The response includes answers generated by the LLAMA model and the source documents from which the information was retrieved.
Getting Started
To set up and run the project:

Clone the Repository:

bash
Copy code
git clone https://github.com/yourusername/document-qa.git
Install Dependencies:

bash
Copy code
pip install -r requirements.txt
Load Data:

Place your textual documents in the data directory.
Run the Application:

bash
Copy code
python main.py
Query the System:

Use the QA chain to input queries and retrieve answers.
Contributing
Feel free to contribute by submitting issues or pull requests. For major changes, please open an issue first to discuss what you would like to change.

License
This project is licensed under the MIT License - see the LICENSE file for details.
